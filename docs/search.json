[
  {
    "objectID": "projects/school/MGTA495/hw3.html",
    "href": "projects/school/MGTA495/hw3.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/school/MGTA495/hw3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/school/MGTA495/hw3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/school/MGTA495/hw3.html#simulate-conjoint-data",
    "href": "projects/school/MGTA495/hw3.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "projects/school/MGTA495/hw3.html#preparing-the-data-for-estimation",
    "href": "projects/school/MGTA495/hw3.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nimport pandas as pd\nimport numpy as np\nimport math\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom cycler import cycler\n\ndf = pd.read_csv('data/conjoint_data.csv')\ndfe = pd.get_dummies(df, columns=['brand'], drop_first=True)\ndfe['ad'] = dfe['ad'] == 'Yes'"
  },
  {
    "objectID": "projects/school/MGTA495/hw3.html#estimation-via-maximum-likelihood",
    "href": "projects/school/MGTA495/hw3.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\nci = 0.95\nX = dfe[['brand_N', 'brand_P', 'ad', 'price']].values\ny = dfe['choice'].values\ngroups = (dfe['resp'].astype(str) + '-' + dfe['task'].astype(str)).factorize()[0]\n\ndef log_likelihood(beta, X, y, groups):\n    ll = 0.0\n    for g in np.unique(groups):\n        idx = groups == g\n        Xg = X[idx]\n        yg = y[idx]\n        U = np.dot(Xg, beta)\n        maxU = max(U) \n        expU = [math.exp(u - maxU) for u in U]\n        denom = sum(expU)\n        probs = [eu / denom for eu in expU]\n        chosen_idx = np.argmax(yg)\n        ll += math.log(probs[chosen_idx])\n    return -ll\n\ninit_beta = np.zeros(X.shape[1])\nres = minimize(log_likelihood, init_beta, args=(X, y, groups), method='BFGS')\n\nbeta_hat = res.x\nhessian_inv = res.hess_inv\nse = np.sqrt(np.diag(hessian_inv))\n\nz = norm.ppf(1-((1-ci)/2))\nci_lower = beta_hat - z * se\nci_upper = beta_hat + z * se\n\nfor name, b, s, low, high in zip(['brand_N', 'brand_P', 'ad', 'price'], beta_hat, se, ci_lower, ci_upper):\n    print(f'{name}: {b:.4f} ± {z * s:.4f} → CI: [{low:.4f}, {high:.4f}]')\n\nbrand_N: 0.9412 ± 0.0395 → CI: [0.9017, 0.9807]\nbrand_P: 0.5016 ± 0.0210 → CI: [0.4806, 0.5226]\nad: -0.7320 ± 0.0117 → CI: [-0.7437, -0.7203]\nprice: -0.0995 ± 0.0120 → CI: [-0.1115, -0.0875]"
  },
  {
    "objectID": "projects/school/MGTA495/hw3.html#estimation-via-bayesian-methods",
    "href": "projects/school/MGTA495/hw3.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\ndef log_prior(beta):\n    lp = 0.0\n    lp += sum(norm.logpdf(beta[:3], loc=0, scale=np.sqrt(5)))  # binary variables\n    lp += norm.logpdf(beta[3], loc=0, scale=1)                # price\n    return lp\n\ndef log_posterior(beta, X, y, groups):\n    return -log_likelihood(beta, X, y, groups) + log_prior(beta)\n\nn_draws = 11000\nbeta_draws = np.zeros((n_draws, 4))\nbeta_current = np.zeros(4)\nlogp_current = log_posterior(beta_current, X, y, groups)\nproposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n\nfor i in range(n_draws):\n    proposal = beta_current + np.random.normal(0, proposal_sd)\n    logp_proposal = log_posterior(proposal, X, y, groups)\n    accept_prob = np.exp(logp_proposal - logp_current)\n    if np.random.rand() &lt; accept_prob:\n        beta_current = proposal\n        logp_current = logp_proposal\n    beta_draws[i, :] = beta_current\n\nbeta_posterior = beta_draws[1000:]\n\nmeans = beta_posterior.mean(axis=0)\nsds = beta_posterior.std(axis=0)\nci_lower = np.percentile(beta_posterior, 2.5, axis=0)\nci_upper = np.percentile(beta_posterior, 97.5, axis=0)\n\nfor name, m, s, lo, hi in zip(['brand_N', 'brand_P', 'ad', 'price'], means, sds, ci_lower, ci_upper):\n    print(f'{name}: {m:.4f} ± {s:.4f} → CI: [{lo:.4f}, {hi:.4f}]')\n\nbrand_N: 0.9403 ± 0.1118 → CI: [0.7230, 1.1737]\nbrand_P: 0.4953 ± 0.1120 → CI: [0.2733, 0.7158]\nad: -0.7295 ± 0.0855 → CI: [-0.9019, -0.5603]\nprice: -0.0995 ± 0.0062 → CI: [-0.1123, -0.0874]\n\nplt.rcParams['axes.prop_cycle'] = cycler(color=['#375a7f']) \nplt.plot(beta_posterior[:,3])\nplt.title(\"Trace plot for price\")\nplt.show()\n\n\n\n\n\n\n\nplt.hist(beta_posterior[:,3], bins=50)\nplt.title(\"Posterior of price\")\nplt.show()    \n\n\n\n\n\n\n\n\nThe Bayesian and MLE estimates for price are nearly identical, but the Bayesian credible intervals are wider for the binary variables, reflecting more uncertainty. The trace plot shows good mixing and the posterior histogram is roughly normal, indicating a stable and reliable MCMC sampling for the price coefficient."
  },
  {
    "objectID": "projects/school/MGTA495/hw3.html#discussion",
    "href": "projects/school/MGTA495/hw3.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nWere this data real rather than simulated, we could conclude a few things from our parameter estimates including that consumers prefer Netflix over Amazon Prime, since \\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}}\\). A negative \\(\\beta_{\\text{price}}\\) indicates that as price increases, the likelihood of a consumer choosing a streaming service decreases, which aligns with standard economic intuition about price sensitivity.\nTo move to a hierarchical model, we would simulate each respondent’s preferences by drawing their betas from a population-level normal distribution. To estimate the model, we’d use hierarchical Bayesian methods that jointly estimate individual-level betas and the overall population parameters."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Last updated 04/05/2025"
  },
  {
    "objectID": "code/code_test.html",
    "href": "code/code_test.html",
    "title": "Wesley Covey",
    "section": "",
    "text": "import hashlib\n\npw = 'bigbear'\nhpw = hashlib.sha256(pw.encode()).hexdigest()\nprint(hpw)\n\nf98f3d8cbcfa62aac83f1abb6411e40c5da9b2cf9ed67c0227b8675351a04500"
  },
  {
    "objectID": "projects/school/example2.html",
    "href": "projects/school/example2.html",
    "title": "Example 2",
    "section": "",
    "text": "##Example 2\nExample histogram below:\nThis is a protected section. Here’s a Python-generated histogram:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.random.rand(100)\nplt.hist(x)\nplt.show()"
  },
  {
    "objectID": "projects/school/MGTA495/hw2.html",
    "href": "projects/school/MGTA495/hw2.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n[1] \"Non-Customer mean:  3.4730127576055\"\n\n\n[1] \"Customer mean:  4.13305613305613\"\n\n\n\n\n\n\n\n\n\nThe histograms of patent counts by customers vs non-customers reveal nearly identical distributions with a slightly higher mean value for customers of the firm.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe company’s customers are disproportionately from the Northeast region, though not entirely. The distibution of age is, howver, much more even for customer vs non-customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n                    Estimate    Std_Error\n(Intercept)     -0.125735914 0.1122180346\nage              0.115793715 0.0063574229\nage_sq          -0.002228748 0.0000771291\nregionNortheast -0.024556782 0.0433762879\nregionNorthwest -0.034827790 0.0529311002\nregionSouth     -0.005441860 0.0524007440\nregionSouthwest -0.037784109 0.0471722463\niscustomer       0.060665584 0.0320588299"
  },
  {
    "objectID": "projects/school/MGTA495/hw2.html#blueprinty-case-study",
    "href": "projects/school/MGTA495/hw2.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n[1] \"Non-Customer mean:  3.4730127576055\"\n\n\n[1] \"Customer mean:  4.13305613305613\"\n\n\n\n\n\n\n\n\n\nThe histograms of patent counts by customers vs non-customers reveal nearly identical distributions with a slightly higher mean value for customers of the firm.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe company’s customers are disproportionately from the Northeast region, though not entirely. The distibution of age is, howver, much more even for customer vs non-customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n                    Estimate    Std_Error\n(Intercept)     -0.125735914 0.1122180346\nage              0.115793715 0.0063574229\nage_sq          -0.002228748 0.0000771291\nregionNortheast -0.024556782 0.0433762879\nregionNorthwest -0.034827790 0.0529311002\nregionSouth     -0.005441860 0.0524007440\nregionSouthwest -0.037784109 0.0471722463\niscustomer       0.060665584 0.0320588299"
  },
  {
    "objectID": "projects/school/MGTA495/hw2.html#airbnb-case-study",
    "href": "projects/school/MGTA495/hw2.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\n       X               id                days       last_scraped      \n Min.   :    1   Min.   :    2515   Min.   :    1   Length:40628      \n 1st Qu.:10158   1st Qu.: 4889868   1st Qu.:  542   Class :character  \n Median :20314   Median : 9862878   Median :  996   Mode  :character  \n Mean   :20314   Mean   : 9698889   Mean   : 1102                     \n 3rd Qu.:30471   3rd Qu.:14667894   3rd Qu.: 1535                     \n Max.   :40628   Max.   :18009669   Max.   :42828                     \n                                                                      \n  host_since         room_type           bathrooms        bedrooms     \n Length:40628       Length:40628       Min.   :0.000   Min.   : 0.000  \n Class :character   Class :character   1st Qu.:1.000   1st Qu.: 1.000  \n Mode  :character   Mode  :character   Median :1.000   Median : 1.000  \n                                       Mean   :1.125   Mean   : 1.147  \n                                       3rd Qu.:1.000   3rd Qu.: 1.000  \n                                       Max.   :8.000   Max.   :10.000  \n                                       NA's   :160     NA's   :76      \n     price         number_of_reviews review_scores_cleanliness\n Min.   :   10.0   Min.   :  0.0     Min.   : 2.000           \n 1st Qu.:   70.0   1st Qu.:  1.0     1st Qu.: 9.000           \n Median :  100.0   Median :  4.0     Median :10.000           \n Mean   :  144.8   Mean   : 15.9     Mean   : 9.198           \n 3rd Qu.:  170.0   3rd Qu.: 17.0     3rd Qu.:10.000           \n Max.   :10000.0   Max.   :421.0     Max.   :10.000           \n                                     NA's   :10195            \n review_scores_location review_scores_value instant_bookable  \n Min.   : 2.000         Min.   : 2.000      Length:40628      \n 1st Qu.: 9.000         1st Qu.: 9.000      Class :character  \n Median :10.000         Median :10.000      Mode  :character  \n Mean   : 9.414         Mean   : 9.332                        \n 3rd Qu.:10.000         3rd Qu.:10.000                        \n Max.   :10.000         Max.   :10.000                        \n NA's   :10254          NA's   :10256                         \n\n\n\nCall:\nglm(formula = number_of_reviews ~ room_type + bathrooms + bedrooms + \n    price + review_scores_cleanliness + review_scores_location + \n    review_scores_value + instant_bookable, family = poisson(), \n    data = df_clean)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.572e+00  1.600e-02 223.215  &lt; 2e-16 ***\nroom_typePrivate room     -1.453e-02  2.737e-03  -5.310 1.09e-07 ***\nroom_typeShared room      -2.519e-01  8.618e-03 -29.229  &lt; 2e-16 ***\nbathrooms                 -1.240e-01  3.747e-03 -33.091  &lt; 2e-16 ***\nbedrooms                   7.494e-02  1.988e-03  37.698  &lt; 2e-16 ***\nprice                     -1.436e-05  8.303e-06  -1.729   0.0838 .  \nreview_scores_cleanliness  1.132e-01  1.493e-03  75.821  &lt; 2e-16 ***\nreview_scores_location    -7.680e-02  1.607e-03 -47.796  &lt; 2e-16 ***\nreview_scores_value       -9.153e-02  1.798e-03 -50.902  &lt; 2e-16 ***\ninstant_bookablet          3.344e-01  2.889e-03 115.748  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 936528  on 30150  degrees of freedom\nAIC: 1058014\n\nNumber of Fisher Scoring iterations: 6\n\n\nInstant booking and higher cleanliness scores both have positive coefficients, leading to more reviews. Shared rooms and higher number of bathrooms both have negative coefficients, resulting in fewer reviews. Surprisingly, higher location and value scores are also linked to fewer reviews, though not as strong as the last two variables. The only variable not statistically significant to the model at the 95% confidence level is price."
  },
  {
    "objectID": "projects/school/MGTA495/hw1.html",
    "href": "projects/school/MGTA495/hw1.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to either be a treatment letter with a matching donation offer or a control letter with no mention of matching donation. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe treatment group was further divided into various matching offer rates, either 1:1, 2:1, or 3:1 dollars matched to dollars donated. Of note, the organization donations were solicited for was a liberal nonprofit organization, and all potential donors mailed were previous donors to this organization. The experiment concluded that the presence of matching donation offers led to increased response rates and increased donation amounts, but no statistically significant increase was observed from higher match ratios.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/school/MGTA495/hw1.html#introduction",
    "href": "projects/school/MGTA495/hw1.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to either be a treatment letter with a matching donation offer or a control letter with no mention of matching donation. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe treatment group was further divided into various matching offer rates, either 1:1, 2:1, or 3:1 dollars matched to dollars donated. Of note, the organization donations were solicited for was a liberal nonprofit organization, and all potential donors mailed were previous donors to this organization. The experiment concluded that the presence of matching donation offers led to increased response rates and increased donation amounts, but no statistically significant increase was observed from higher match ratios.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/school/MGTA495/hw1.html#data",
    "href": "projects/school/MGTA495/hw1.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Summary\n\n\n\n\n\n\ndf = pd.read_stata('data/karlan_list_2007.dta')\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ntreatment\n50083.0\n0.666813\n0.471357\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\ncontrol\n50083.0\n0.333187\n0.471357\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nratio2\n50083.0\n0.222311\n0.415803\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nratio3\n50083.0\n0.222211\n0.415736\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsize25\n50083.0\n0.166723\n0.372732\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsize50\n50083.0\n0.166623\n0.372643\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsize100\n50083.0\n0.166723\n0.372732\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsizeno\n50083.0\n0.166743\n0.372750\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\naskd1\n50083.0\n0.222311\n0.415803\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\naskd2\n50083.0\n0.222291\n0.415790\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\naskd3\n50083.0\n0.222211\n0.415736\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nask1\n50083.0\n71.501807\n101.728936\n25.000000\n35.000000\n45.000000\n65.000000\n1500.000000\n\n\nask2\n50083.0\n91.792724\n127.252628\n35.000000\n45.000000\n60.000000\n85.000000\n1875.000000\n\n\nask3\n50083.0\n111.046263\n151.673562\n50.000000\n55.000000\n70.000000\n100.000000\n2250.000000\n\n\namount\n50083.0\n0.915694\n8.707393\n0.000000\n0.000000\n0.000000\n0.000000\n400.000000\n\n\ngave\n50083.0\n0.020646\n0.142197\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\namountchange\n50083.0\n-52.672016\n1267.097656\n-200412.125000\n-50.000000\n-30.000000\n-25.000000\n275.000000\n\n\nhpa\n50083.0\n59.384975\n71.179871\n0.000000\n30.000000\n45.000000\n60.000000\n1000.000000\n\n\nltmedmra\n50083.0\n0.493720\n0.499966\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nfreq\n50083.0\n8.039355\n11.394454\n0.000000\n2.000000\n4.000000\n10.000000\n218.000000\n\n\nyears\n50082.0\n6.097540\n5.503492\n0.000000\n2.000000\n5.000000\n9.000000\n95.000000\n\n\nyear5\n50083.0\n0.508815\n0.499927\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nmrm2\n50082.0\n13.007268\n12.081403\n0.000000\n4.000000\n8.000000\n19.000000\n168.000000\n\n\ndormant\n50083.0\n0.523471\n0.499454\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nfemale\n48972.0\n0.277669\n0.447854\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\ncouple\n48935.0\n0.091897\n0.288884\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nstate50one\n50083.0\n0.000998\n0.031581\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nnonlit\n49631.0\n2.473918\n1.961528\n0.000000\n1.000000\n3.000000\n4.000000\n6.000000\n\n\ncases\n49631.0\n1.499768\n1.155140\n0.000000\n1.000000\n1.000000\n2.000000\n4.000000\n\n\nstatecnt\n50083.0\n5.998820\n5.745993\n0.001995\n1.833234\n3.538799\n9.607021\n17.368841\n\n\nstateresponse\n50083.0\n0.020627\n0.005171\n0.000000\n0.018163\n0.019710\n0.023048\n0.076923\n\n\nstateresponset\n50083.0\n0.021989\n0.006257\n0.000000\n0.018493\n0.021697\n0.024703\n0.111111\n\n\nstateresponsec\n50080.0\n0.017717\n0.007516\n0.000000\n0.012862\n0.019881\n0.020806\n0.052632\n\n\nstateresponsetminc\n50080.0\n0.004273\n0.009112\n-0.047619\n-0.001388\n0.001779\n0.010545\n0.111111\n\n\nperbush\n50048.0\n0.487940\n0.078733\n0.090909\n0.444444\n0.484848\n0.525253\n0.731959\n\n\nclose25\n50048.0\n0.185702\n0.388870\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nred0\n50048.0\n0.404452\n0.490791\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nblue0\n50048.0\n0.595548\n0.490791\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nredcty\n49978.0\n0.510245\n0.499900\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nbluecty\n49978.0\n0.488715\n0.499878\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\npwhite\n48217.0\n0.819599\n0.168560\n0.009418\n0.755845\n0.872797\n0.938827\n1.000000\n\n\npblack\n48047.0\n0.086710\n0.135868\n0.000000\n0.014729\n0.036554\n0.090882\n0.989622\n\n\npage18_39\n48217.0\n0.321694\n0.103039\n0.000000\n0.258311\n0.305534\n0.369132\n0.997544\n\n\nave_hh_sz\n48221.0\n2.429012\n0.378105\n0.000000\n2.210000\n2.440000\n2.660000\n5.270000\n\n\nmedian_hhincome\n48209.0\n54815.700533\n22027.316665\n5000.000000\n39181.000000\n50673.000000\n66005.000000\n200001.000000\n\n\npowner\n48214.0\n0.669418\n0.193405\n0.000000\n0.560222\n0.712296\n0.816798\n1.000000\n\n\npsch_atlstba\n48215.0\n0.391661\n0.186599\n0.000000\n0.235647\n0.373744\n0.530036\n1.000000\n\n\npop_propurban\n48217.0\n0.871968\n0.258633\n0.000000\n0.884929\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\ntest_variables = ['hpa', 'freq', 'years', 'median_hhincome']\n\nfor var in test_variables:\n    control = df.loc[df.treatment == 0, var].dropna()\n    treatment = df.loc[df.treatment == 1, var].dropna()\n\n    t_stat_manual = np.divide(\n        (control.mean() - treatment.mean()),\n        np.sqrt(\n            (treatment.var(ddof=1) / len(treatment)) +\n            (control.var(ddof=1) / len(control))\n        )\n    )\n    pval_manual = 2 * (1 - t.cdf(np.abs(t_stat_manual), df=len(control) + len(treatment) - 2))\n\n    t_stat, pval = ttest_ind(control, treatment)\n    print(f'''\nT-test Results for {var}:\n\nControl mean: {control.mean()}\nTreatment mean: {treatment.mean()}\n\nt-statistic (manual): {t_stat_manual}\nt-statistic (scipy): {t_stat}\np-value (manual): {pval_manual}\np-value (scipy): {pval}\n''')\n    m = rsm.model.regress(data=df,\n                      rvar=var,\n                      evar=['treatment'],\n    )\n    print(f'Linear Regression Results for {var}:')\n\n    print(m.summary(main=False))\n    print(f'''\nAt the 95% confidence level, we {pval &lt; 0.05 and \"reject\" or \"fail to reject\"} the null hypothesis \nthat the mean value from the two samples are equal.\n---------------------------------------------------''')\n\n\nT-test Results for hpa:\n\nControl mean: 58.960166931152344\nTreatment mean: 59.59724044799805\n\nt-statistic (manual): -0.9703896722043864\nt-statistic (scipy): -0.944145044786662\np-value (manual): 0.33185698112371353\np-value (scipy): 0.34510008823759086\n\nLinear Regression Results for hpa:\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.891 df(1, 50081), p.value 0.345\nNr obs: 50,083\nNone\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the mean value from the two samples are equal.\n---------------------------------------------------\n\nT-test Results for freq:\n\nControl mean: 8.047342242464193\nTreatment mean: 8.035363516588813\n\nt-statistic (manual): 0.11084502380904246\nt-statistic (scipy): 0.11089297035979982\np-value (manual): 0.9117396856546793\np-value (scipy): 0.9117016644344591\n\nLinear Regression Results for freq:\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.012 df(1, 50081), p.value 0.912\nNr obs: 50,083\nNone\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the mean value from the two samples are equal.\n---------------------------------------------------\n\nT-test Results for years:\n\nControl mean: 6.1359141846946725\nTreatment mean: 6.078365024704297\n\nt-statistic (manual): 1.0909175279573782\nt-statistic (scipy): 1.103038374578911\np-value (manual): 0.2753144222756161\np-value (scipy): 0.27001580108724454\n\nLinear Regression Results for years:\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 1.217 df(1, 50080), p.value 0.27\nNr obs: 50,082\nNone\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the mean value from the two samples are equal.\n---------------------------------------------------\n\nT-test Results for median_hhincome:\n\nControl mean: 54921.09447493141\nTreatment mean: 54763.168992633575\n\nt-statistic (manual): 0.7432960510660361\nt-statistic (scipy): 0.741683012117828\np-value (manual): 0.4573060841853336\np-value (scipy): 0.458283028000566\n\nLinear Regression Results for median_hhincome:\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.55 df(1, 48207), p.value 0.458\nNr obs: 48,209\nNone\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the mean value from the two samples are equal.\n---------------------------------------------------\n\n\nConfirmed through manual t-test calculation, t-tests performed with scipy, and with linear regression models, none of the assessed variables are statistically significantly different between the control and treatment groups."
  },
  {
    "objectID": "projects/school/MGTA495/hw1.html#experimental-results",
    "href": "projects/school/MGTA495/hw1.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\ngave_df = df.groupby('treatment')['gave'].mean()\ngave_df.index = gave_df.index.map({0: 'Control', 1: 'Treatment'})\n\nplt.bar(gave_df.index, gave_df.values)\nplt.title('Proportion Who Donated by Group')\nplt.xlabel('Group')\nplt.ylabel('Proportion Who Donated')\nfor i, v in enumerate(gave_df.values):\n    plt.text(i, v-0.001, f\"{v:.3f}\", ha='center', color='white')\nplt.show()\n\n\n\n\n\n\n\n\n\ncontrol = df.loc[df.treatment == 0, 'gave'].dropna()\ntreatment = df.loc[df.treatment == 1, 'gave'].dropna()\n\nt_stat, pval = ttest_ind(control, treatment)\n\nprint(f'''\nT-test Results: \n\nControl mean: {control.mean()}\nTreatment mean: {treatment.mean()}\n\nt-statistic: {t_stat}\np-value: {pval}''')\n\nprobit_model = smf.probit('gave ~ treatment', data=df).fit(disp=False)\nprint(f'''\nProbit Regression Results:\n      \nt-statistic: {probit_model.tvalues['treatment']}\np-value: {probit_model.pvalues['treatment']}\n\nAt the 95% confidence level, we {pval &lt; 0.05 and \"reject\" or \"fail to reject\"} the null hypothesis \nthat the mean value from the two samples are equal.\n''')\n\n\nT-test Results: \n\nControl mean: 0.017858212980164198\nTreatment mean: 0.02203856749311295\n\nt-statistic: -3.101361000543946\np-value: 0.0019274025949016982\n\nProbit Regression Results:\n      \nt-statistic: 3.1129300737950434\np-value: 0.0018523990147782177\n\nAt the 95% confidence level, we reject the null hypothesis \nthat the mean value from the two samples are equal.\n\n\n\nFrom both the t-test and the probit regression model, we find a statistically significant difference between the response rates (average value for binary ‘gave’ variable) from the control and treatment samples at the 99% confidence level. From this, we can conclude that potential donors are more likely to respond to solicitation and donate when provided an offer of matching donation contributions.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ndf['ratio'] = df['ratio'].astype(str)\ndf['ratio'] = pd.Categorical(\n    df['ratio'], categories=['Control', '1', '2', '3'], ordered=True\n)\nratio_df = df.groupby('ratio', observed=False)['gave'].mean()\n\nratios = ratio_df.index.values[1:]\nfor i in range(len((ratios))):\n\n    r1 = ratios[i]\n    r2 = ratios[i + 1] if i + 1 &lt; len(ratios) else ratios[0]\n\n    group1 = df.loc[df.ratio == r1, 'gave'].dropna()\n    group2 = df.loc[df.ratio == r2, 'gave'].dropna()\n\n    t_stat, pval = ttest_ind(group1, group2)\n    print(f'''\nResponse rate for {r1}:1  {group1.mean()}\nResponse rate for {r2}:1  {group2.mean()}\n\nt-statistic: {t_stat}\np-value: {pval}\n\nAt the 95% confidence level, we {pval &lt; 0.05 and \"reject\" or \"fail to reject\"} the null hypothesis \nthat the response rate of ratios {r1}:1 and {r2}:1 are equal.\n---------------------------------------------------''')\n\n\nResponse rate for 1:1  0.020749124225276205\nResponse rate for 2:1  0.0226333752469912\n\nt-statistic: -0.96504713432247\np-value: 0.33453168549723933\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the response rate of ratios 1:1 and 2:1 are equal.\n---------------------------------------------------\n\nResponse rate for 2:1  0.0226333752469912\nResponse rate for 3:1  0.022733399227244138\n\nt-statistic: -0.05011583793874515\np-value: 0.9600305283739325\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the response rate of ratios 2:1 and 3:1 are equal.\n---------------------------------------------------\n\nResponse rate for 3:1  0.022733399227244138\nResponse rate for 1:1  0.020749124225276205\n\nt-statistic: 1.0150255853798622\np-value: 0.3101046637086672\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the response rate of ratios 3:1 and 1:1 are equal.\n---------------------------------------------------\n\n\nAs seen through the above series of t-tests, comparing the mean response rates between varying match ratio samples, increasing match ratios above 1:1 has no significant impact on response rates. This affirms the conclusion made by the authors of the study.\n\nResponse Rate Regressed on Match Ratio (from full dataset):\n\nprobit_model2 = smf.probit('gave ~ ratio', data=df).fit(disp=False)\n\npm2_df = pd.DataFrame({\n    'coef': probit_model2.params,\n    't-stat': probit_model2.tvalues,\n    'pval': probit_model2.pvalues\n})\ndisplay(pm2_df)\n\n\n\n\n\n\n\n\ncoef\nt-stat\npval\n\n\n\n\nIntercept\n-2.100141\n-90.072770\n0.000000\n\n\nratio[T.1]\n0.061624\n1.725748\n0.084393\n\n\nratio[T.2]\n0.097974\n2.792255\n0.005234\n\n\nratio[T.3]\n0.099831\n2.847311\n0.004409\n\n\n\n\n\n\n\nThe large negative coefficient on the intercept demonstrates the relatively low probability of response for the control group, where the positive coefficients combined with low p-values, particularly for 2:1 and 3:1 match rates, demonstrate the statistically significant increase that the treatment has on respones rates. To further asses the difference between match rates, I will run the regression again, excluding the control group.\n\n\nResponse Rate Regressed on Match Ratio (from treatment group only):\n\ntreatment_df = df.loc[df.treatment == 1]\ntreatment_df = treatment_df[['gave', 'ratio']].dropna()\ntreatment_df['ratio'] = pd.Categorical(\n    treatment_df['ratio'], categories=['1', '2', '3'], ordered=True\n)\n\nprobit_model3 = smf.probit('gave ~ ratio', data=treatment_df).fit(disp=False)\n\npm3_df = pd.DataFrame({\n    'coef': probit_model3.params,\n    't-stat': probit_model3.tvalues,\n    'pval': probit_model3.pvalues\n})\ndisplay(pm3_df)\n\n\n\n\n\n\n\n\ncoef\nt-stat\npval\n\n\n\n\nIntercept\n-2.038517\n-75.373213\n0.000000\n\n\nratio[T.2]\n0.036350\n0.964972\n0.334559\n\n\nratio[T.3]\n0.038207\n1.014933\n0.310138\n\n\n\n\n\n\n\nWith the control group excluded, we can see from the high p-values on the coefficients for 2:1 and 3:1 match ratios that they do not have a significantly different impact on response rates from the intercept (1:1).\n\nprint(f'Observed difference between 3:1 response rate and 2:1 response rate: {ratio_df['3'] - ratio_df['2']}')\nprint(f'Observed difference between 2:1 response rate and 1:1 response rate: {ratio_df['2'] - ratio_df['1']}')\n\nmfx = probit_model2.get_margeff()\nmarginal_effects = mfx.margeff\ndiff_3_vs_2 = marginal_effects[2] - marginal_effects[1]\ndiff_2_vs_1 = marginal_effects[1] - marginal_effects[0]\n\nprint(f\"Estimated difference between 3:1 response rate and 2:1 response rate: {diff_3_vs_2}\")\nprint(f\"Estimated difference between 2:1 response rate and 1:1 response rate: {diff_2_vs_1}\")\n\nObserved difference between 3:1 response rate and 2:1 response rate: 0.00010002398025293902\nObserved difference between 2:1 response rate and 1:1 response rate: 0.0018842510217149944\nEstimated difference between 3:1 response rate and 2:1 response rate: 9.229335508974864e-05\nEstimated difference between 2:1 response rate and 1:1 response rate: 0.0018064011975636126\n\n\nThe analysis continues to affirm the authors’ findings that beyond adding a matching donation of any size, increasing match ratios is not an effective method of driving higher response rates.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\ncontrol = df.loc[df.treatment == 0, 'amount'].dropna()\ntreatment = df.loc[df.treatment == 1, 'amount'].dropna()\n\nt_stat, pval = ttest_ind(control, treatment)\n\nprint(f'''\nT-test Results:\n      \nControl mean: ${control.mean():,.2f}\nTreatment mean: ${treatment.mean():,.2f}\n      \nt-statistic: {t_stat}\np-value: {pval}\n\nAt the 95% confidence level, we {pval &lt; 0.05 and \"reject\" or \"fail to reject\"} the null hypothesis \nthat the mean donation amount of the two samples are equal.''')\n\n\nT-test Results:\n      \nControl mean: $0.81\nTreatment mean: $0.97\n      \nt-statistic: -1.8605020225753781\np-value: 0.06282038947470686\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the mean donation amount of the two samples are equal.\n\n\nWe can observe that the treatment group has a higher mean donation amount, as identified by the authors, but at the 95% confidence level, it is not statistically significant.\n\ncontrol = df.loc[(df.treatment == 0) & (df.gave == 1), 'amount'].dropna()\ntreatment = df.loc[(df.treatment == 1) & (df.gave == 1), 'amount'].dropna()\n\nt_stat, pval = ttest_ind(control, treatment)\n\nprint(f'''\nT-test Results:\n      \nControl mean: ${control.mean():,.2f}\nTreatment mean: ${treatment.mean():,.2f}\n\nt-statistic: {t_stat}\np-value: {pval}\n\nAt the 95% confidence level, we {pval &lt; 0.05 and \"reject\" or \"fail to reject\"} the null hypothesis \nthat the donation amount of the two samples are equal.''')\n\n\nT-test Results:\n      \nControl mean: $45.54\nTreatment mean: $43.87\n\nt-statistic: 0.5808388615237938\np-value: 0.5614758782284279\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the donation amount of the two samples are equal.\n\n\nAssessing only those who made a donation, we in fact see that the mean donation amount among the treatment group is lower than the control group, though with a much higher p-value than the previous t-test. It appears that while the treatment results in higher response rates, most of those donors who are ‘converted’ by the treatment are making smaller donations than those who donated from the control group.\n\nfig, axes = plt.subplots(1, 2, figsize=(7.8, 3.9), sharey=True)\naxes[0].set_ylabel(\"Frequency\")\n\nfor val in [0, 1]:\n    subset = df.loc[(df.gave == 1) & (df.treatment == val)].copy()\n    axes[val].hist(subset['amount'], bins=10)\n    axes[val].axvline(x=subset['amount'].mean(), color='r', linestyle='--', label=f'Mean: ${subset[\"amount\"].mean():,.2f}')\n    axes[val].legend()\n    axes[val].set_title(f'Amount Donated for {\"Treatment\" if val == 1 else \"Control\"} Group')\n    axes[val].set_xlabel(\"Dollars Donated\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/school/MGTA495/hw1.html#simulation-experiment",
    "href": "projects/school/MGTA495/hw1.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThe below chart illustrates the cumulative average difference in response rates from 10,000 draws of simulated treatment and control distributions. In early simulations, we see this cumulative average move erratically due to random variation, but as more draws are simulated, the average begins to stabilize and converge toward the true observed treatment effect of approximately 0.004.\n\nctr_p = 0.018\ntrt_p = 0.022\n\nnp.random.seed(12)\nsim_ctr = np.random.binomial(n=1, p=ctr_p, size=10_000)\nsim_trt = np.random.binomial(n=1, p=trt_p, size=10_000)\n\nsim_diff = sim_trt - sim_ctr\ncumulative_avg = np.cumsum(sim_diff) / np.arange(1, len(sim_diff) + 1)\n\nplt.figure(figsize=(7.8, 3.9))\nplt.plot(cumulative_avg)\nplt.axhline(y=trt_p - ctr_p, color='red', linestyle='--', label=f'True Effect ({trt_p-ctr_p:.3f})')\nplt.title('Cumulative Average of Simulated Treatment Effect')\nplt.xlabel('Number of Simulated Draws')\nplt.ylabel('Cum. Avg. Difference (Treatment - Control)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nThese histograms show the distribution of simulated average differences in response rates between treatment and control groups at sample sizes of 50, 200, 500, and 1000. As the sample size increases, the distributions become tighter and more symmetric, demonstrating the central limit theorem: larger samples yield more stable, normally distributed estimates.\n\nfig, axes = plt.subplots(2, 2, figsize=(7.8, 7.8), sharey=True, sharex=True)\naxes = axes.flatten()\naxes[0].set_ylabel(\"Frequency\")\naxes[2].set_ylabel(\"Frequency\")\naxes[2].set_xlabel(\"Difference in Response Rate\")\naxes[3].set_xlabel(\"Difference in Response Rate\")\n\nfor ax, size in enumerate([50, 200, 500, 1000]):\n    samples = []\n    for i in range(1000):\n        sim_ctr = np.random.binomial(n=1, p=ctr_p, size=size)\n        sim_trt = np.random.binomial(n=1, p=trt_p, size=size)\n\n        sim_diff_mean = sim_trt.mean() - sim_ctr.mean()\n        samples.append(sim_diff_mean)\n    \n    axes[ax].hist(samples, bins=np.linspace(-0.02, 0.03, 11))\n    axes[ax].axvline(x=trt_p - ctr_p, color='red', linestyle='--', label='True Effect (0.004)')\n    axes[ax].legend()\n    axes[ax].set_title(f'Average Difference from {size} Draws')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/school/example1.html",
    "href": "projects/school/example1.html",
    "title": "Example 1",
    "section": "",
    "text": "Example dataframe below:\n\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"A\": [1, 2, 3],\n    \"B\": [4, 5, 6]\n})\n\nprint(df)\n\n   A  B\n0  1  4\n1  2  5\n2  3  6"
  },
  {
    "objectID": "code/old_code.html",
    "href": "code/old_code.html",
    "title": "Wesley Covey",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Logos used on this site are the property of their respective owners. No endorsement or affiliation is implied. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\nMay 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSan Diego Real Estate Shiny App\n\n\n\nNov 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Trade Power BI Report\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wesley D. Covey",
    "section": "",
    "text": "Wesley is a data-driven operations analyst and Marine Corps veteran with a proven record of turning complex data into actionable insights. With experience spanning financial analytics, business intelligence, and process optimization, he brings precision and purpose to every challenge. Outside of work, he enjoys gardening, woodworking, and exploring San Diego’s hiking trails.\n\n\nUniversity of California, San Diego\nRady School of Management\nM.S. in Business Analytics | 2024 - Present\nArizona State University\nW.P. Carey School of Business\nB.S. in Economics | 2018 - 2023\n\n\n\nAcademy Securities\nOperations Analyst | 2023 - Present\nUnited States Marine Corps\nChief Force Deployment Analyst | 2015 - 2023"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Wesley D. Covey",
    "section": "",
    "text": "University of California, San Diego\nRady School of Management\nM.S. in Business Analytics | 2024 - Present\nArizona State University\nW.P. Carey School of Business\nB.S. in Economics | 2018 - 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Wesley D. Covey",
    "section": "",
    "text": "Academy Securities\nOperations Analyst | 2023 - Present\nUnited States Marine Corps\nChief Force Deployment Analyst | 2015 - 2023"
  },
  {
    "objectID": "bonus.html",
    "href": "bonus.html",
    "title": "Rex",
    "section": "",
    "text": "🦖🌵\n\n\nScore: 0\n\n\n\n Jump!"
  }
]