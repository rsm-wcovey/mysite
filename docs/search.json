[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Last updated 04/05/2025"
  },
  {
    "objectID": "projects/work/global_trade.html",
    "href": "projects/work/global_trade.html",
    "title": "Global Trade Power BI Report",
    "section": "",
    "text": "Our Macroeconomist was looking for a tool to analyze data on global trade and determine what goods might be most effected as future geopolitical events developed. After assessing various data sources for scope, reliability, and accessibility, I decided to pull data from the UN Comtrade Database. That data was then transformed and related for presentation in the below Power BI report."
  },
  {
    "objectID": "projects/work/global_trade.html#project-notes",
    "href": "projects/work/global_trade.html#project-notes",
    "title": "Global Trade Power BI Report",
    "section": "",
    "text": "Our Macroeconomist was looking for a tool to analyze data on global trade and determine what goods might be most effected as future geopolitical events developed. After assessing various data sources for scope, reliability, and accessibility, I decided to pull data from the UN Comtrade Database. That data was then transformed and related for presentation in the below Power BI report."
  },
  {
    "objectID": "projects/school/MGTA495/hw4.html",
    "href": "projects/school/MGTA495/hw4.html",
    "title": "Rage Against The Machine Learning",
    "section": "",
    "text": "Below, I will demonstrate two forms of machine learning, one unsupervised - K-means, and one supervised - K Nearest Neighbors."
  },
  {
    "objectID": "projects/school/MGTA495/hw4.html#introduction",
    "href": "projects/school/MGTA495/hw4.html#introduction",
    "title": "Rage Against The Machine Learning",
    "section": "",
    "text": "Below, I will demonstrate two forms of machine learning, one unsupervised - K-means, and one supervised - K Nearest Neighbors."
  },
  {
    "objectID": "projects/school/MGTA495/hw4.html#k-means",
    "href": "projects/school/MGTA495/hw4.html#k-means",
    "title": "Rage Against The Machine Learning",
    "section": "K-Means",
    "text": "K-Means\nK-means is an unsupervised clustering algorithm that partitions data into k groups by minimizing the distance between points and their assigned cluster centroids.\nUsing a dataset on penguin feature measurements, I will demonstrate this process with a manual Python implementation that iteratively updates centroids to form well-separated clusters.\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\n\ndef manual_kmeans(arr, k=3, iter=5, rand_seed=42):\n    np.random.seed(rand_seed)\n    centroids = arr[np.random.choice(arr.shape[0], k, replace=False)]\n\n    for n in range(iter): \n        distances = cdist(arr, centroids)\n        labels = np.argmin(distances, axis=1)\n        \n        for i in range(k):\n            centroids[i] = arr[labels == i].mean(axis=0)\n\n    plt.scatter(\n        arr[:, 0], arr[:, 1], \n            c=labels, cmap='Accent', alpha=0.6)\n    plt.scatter(\n        centroids[:, 0], centroids[:, 1], \n            color='red', marker='o')\n\n    plt.xlabel(\"Bill Length (mm)\")\n    plt.ylabel(\"Flipper Length (mm)\")\n    plt.title(\"Manual K-Means Result\")\n    plt.show()\n\n\npenguins = pd.read_csv('data/palmer_penguins.csv')\nquack = penguins.loc[:,['bill_length_mm', 'flipper_length_mm']].dropna().values\n\nAs the number of iterations increases, we can see the centroids moving toward their optimal positions and the clusters shifting, eventually stabilizing.\n\n1 Iteration4 Iterations8 Iterations12 Iterations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor comparison, I will run this dataset through scikit-learn’s built-in K-means function.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nkmeans = KMeans(n_clusters=3, n_init=15, random_state=42)\n\nkmeans.fit(quack)\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n\nplt.scatter(\n        quack[:, 0], quack[:, 1], \n            c=labels, cmap='Accent', alpha=0.6)\nplt.scatter(\n        centroids[:, 0], centroids[:, 1], \n            color='red', marker='o')\n\n\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.title(\"Built-in KMeans Result\")\nplt.show()\n\n\n\n\n\n\n\n\nIn my prior examples, I have chosen to segment the data into k=3 clusters, but this may not be optimal. To assess the number of clusters, we can perform two tests: Within-Cluster Sum of Squares and Silhouette Scores.\n\nWithin-Cluster Sum of Squares\nA low Within-Cluster Sum of Squares (WCSS) is good, indicating that the points in a cluster are close to their centroid. However, as the number of clusters (k) increases, the WCSS will always decrease until eventually reaching zero at k = the number of data points. Of course, having that many clusters is not useful or informative. In the plot below, the WCSS drops sharply at first, then levels off. The “elbow” in the curve, where the rate of improvement slows, appears between k = 3 and k = 6, suggesting a good range for the optimal number of clusters.\n\nwcss = []\nsilhouette = []\nK_range = range(2, 20)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n    labels = kmeans.fit_predict(quack)\n    wcss.append(kmeans.inertia_)\n    silhouette.append(silhouette_score(quack, labels))\n\nplt.plot(K_range, wcss, marker='o')\nplt.title(\"Within-Cluster Sum of Squares\")\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Within-Cluster Sum of Squares\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSilhouette Scores\nIn contrast to WCSS, silhouette scores evaluate a point’s proximity to other points within its cluster as well as to points in other clusters. The closer it is to points in its own cluster and the farther it is from other clusters, the higher the score. Unlike WCSS, increasing the number of clusters does not always lead to better scores — too many clusters can create tightly packed, less meaningful groupings.\n\nplt.plot(K_range, silhouette, marker='o', color='green')\nplt.title(\"Silhouette Scores\")\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Silhouette Score\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nComparing the two measurements, it appears that k = 2 or k = 3 may be optimal. These values significantly reduce WCSS before the silhouette score drops more noticeably at higher values of k."
  },
  {
    "objectID": "projects/school/MGTA495/hw4.html#k-nearest-neighbors",
    "href": "projects/school/MGTA495/hw4.html#k-nearest-neighbors",
    "title": "Rage Against The Machine Learning",
    "section": "K Nearest Neighbors",
    "text": "K Nearest Neighbors\nK-nearest neighbors (KNN) is a simple, non-parametric classification algorithm that predicts a point’s label based on the majority label of its closest k neighbors in the training data.\nWe will first create a synthetic dataset with two input features (x1, x2) and a binary outcome y, which is based on whether a point lies above or below a nonlinear “wiggly” boundary. We’ll use this to train the KNN model.\n\nnp.random.seed(42)\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\nboundary = np.sin(4 * x1) + x1\ny = (x2 &gt; boundary).astype(int)\n\ntrain = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\n\nplt.scatter(train['x1'], train['x2'], c=train['y'], cmap='bwr', edgecolor='k')\nplt.plot(np.sort(x1), np.sin(4 * np.sort(x1)) + np.sort(x1), color='black', linestyle='--', label='Boundary')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Training Data with Wiggly Boundary')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nHere, the training data is visualized in 2D space. Points are colored by class (y), and the wiggly decision boundary is shown as a dashed line for reference.\nBelow, a second synthetic dataset is generated using a different random seed. This test set follows the same logic as the training set and will be used to evaluate model accuracy.\n\nnp.random.seed(99)\nx1_test = np.random.uniform(-3, 3, n)\nx2_test = np.random.uniform(-3, 3, n)\nboundary_test = np.sin(4 * x1_test) + x1_test\ny_test = (x2_test &gt; boundary_test).astype(int)\n\ntest = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})\n\nThis section defines a manual version of the K-nearest neighbors classifier. For each value of k from 1 to 30, we predict labels for the test set and compute the classification accuracy. The resulting plot shows how the accuracy of the manual KNN classifier changes with different values of k. This helps us identify which value of k gives the best test performance.\n\ndef euclidean(a, b):\n    return np.sqrt(np.sum((a - b) ** 2, axis=1))\n\ndef knn_predict(X_train, y_train, X_test, k):\n    preds = []\n    for point in X_test:\n        dists = euclidean(X_train, point)\n        nearest = y_train[np.argsort(dists)[:k]]\n        pred = np.round(np.mean(nearest)).astype(int)  # majority vote\n        preds.append(pred)\n    return np.array(preds)\n\nX_train = train[['x1', 'x2']].to_numpy()\ny_train = train['y'].to_numpy()\nX_test = test[['x1', 'x2']].to_numpy()\ny_true = test['y'].to_numpy()\n\naccuracies = []\n\nfor k in range(1, 31):\n    y_pred = knn_predict(X_train, y_train, X_test, k)\n    acc = np.mean(y_pred == y_true)\n    accuracies.append(acc)\n\nplt.plot(range(1, 31), [a * 100 for a in accuracies], marker='o')\nplt.title(\"KNN Accuracy on Test Set\")\nplt.xlabel(\"k (number of neighbors)\")\nplt.ylabel(\"Accuracy (%)\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe same evaluation is repeated, using scikit-learn’s built-in KNN function, serving as a check to confirm the manual implementation.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\naccuracies_sklearn = []\n\nfor k in range(1, 31):\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc = np.mean(y_pred == y_true)\n    accuracies_sklearn.append(acc)\n\nplt.plot(range(1, 31), [a * 100 for a in accuracies], label='Manual', marker='o')\nplt.plot(range(1, 31), [a * 100 for a in accuracies_sklearn], label='sklearn', marker='x')\nplt.title(\"Manual vs Sklearn KNN Accuracy\")\nplt.xlabel(\"k\")\nplt.ylabel(\"Accuracy (%)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn this case, the highest accuracy was achieved with k = 1 or 2, and accuracy declined as k increased, suggesting that a small number of neighbors gives the best classification performance for this dataset."
  },
  {
    "objectID": "projects/school/MGTA495/hw1.html",
    "href": "projects/school/MGTA495/hw1.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to either be a treatment letter with a matching donation offer or a control letter with no mention of matching donation. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe treatment group was further divided into various matching offer rates, either 1:1, 2:1, or 3:1 dollars matched to dollars donated. Of note, the organization donations were solicited for was a liberal nonprofit organization, and all potential donors mailed were previous donors to this organization. The experiment concluded that the presence of matching donation offers led to increased response rates and increased donation amounts, but no statistically significant increase was observed from higher match ratios.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/school/MGTA495/hw1.html#introduction",
    "href": "projects/school/MGTA495/hw1.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to either be a treatment letter with a matching donation offer or a control letter with no mention of matching donation. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe treatment group was further divided into various matching offer rates, either 1:1, 2:1, or 3:1 dollars matched to dollars donated. Of note, the organization donations were solicited for was a liberal nonprofit organization, and all potential donors mailed were previous donors to this organization. The experiment concluded that the presence of matching donation offers led to increased response rates and increased donation amounts, but no statistically significant increase was observed from higher match ratios.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/school/MGTA495/hw1.html#data",
    "href": "projects/school/MGTA495/hw1.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Summary\n\n\n\n\n\n\ndf = pd.read_stata('data/karlan_list_2007.dta')\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ntreatment\n50083.0\n0.666813\n0.471357\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\ncontrol\n50083.0\n0.333187\n0.471357\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nratio2\n50083.0\n0.222311\n0.415803\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nratio3\n50083.0\n0.222211\n0.415736\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsize25\n50083.0\n0.166723\n0.372732\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsize50\n50083.0\n0.166623\n0.372643\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsize100\n50083.0\n0.166723\n0.372732\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nsizeno\n50083.0\n0.166743\n0.372750\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\naskd1\n50083.0\n0.222311\n0.415803\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\naskd2\n50083.0\n0.222291\n0.415790\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\naskd3\n50083.0\n0.222211\n0.415736\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nask1\n50083.0\n71.501807\n101.728936\n25.000000\n35.000000\n45.000000\n65.000000\n1500.000000\n\n\nask2\n50083.0\n91.792724\n127.252628\n35.000000\n45.000000\n60.000000\n85.000000\n1875.000000\n\n\nask3\n50083.0\n111.046263\n151.673562\n50.000000\n55.000000\n70.000000\n100.000000\n2250.000000\n\n\namount\n50083.0\n0.915694\n8.709199\n0.000000\n0.000000\n0.000000\n0.000000\n400.000000\n\n\ngave\n50083.0\n0.020646\n0.142197\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\namountchange\n50083.0\n-52.672016\n1267.238647\n-200412.125000\n-50.000000\n-30.000000\n-25.000000\n275.000000\n\n\nhpa\n50083.0\n59.384975\n71.177307\n0.000000\n30.000000\n45.000000\n60.000000\n1000.000000\n\n\nltmedmra\n50083.0\n0.493720\n0.499966\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nfreq\n50083.0\n8.039355\n11.394454\n0.000000\n2.000000\n4.000000\n10.000000\n218.000000\n\n\nyears\n50082.0\n6.097540\n5.503492\n0.000000\n2.000000\n5.000000\n9.000000\n95.000000\n\n\nyear5\n50083.0\n0.508815\n0.499927\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nmrm2\n50082.0\n13.007268\n12.081403\n0.000000\n4.000000\n8.000000\n19.000000\n168.000000\n\n\ndormant\n50083.0\n0.523471\n0.499454\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nfemale\n48972.0\n0.277669\n0.447854\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\ncouple\n48935.0\n0.091897\n0.288884\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nstate50one\n50083.0\n0.000998\n0.031581\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nnonlit\n49631.0\n2.473918\n1.961528\n0.000000\n1.000000\n3.000000\n4.000000\n6.000000\n\n\ncases\n49631.0\n1.499768\n1.155140\n0.000000\n1.000000\n1.000000\n2.000000\n4.000000\n\n\nstatecnt\n50083.0\n5.998820\n5.746324\n0.001995\n1.833234\n3.538799\n9.607021\n17.368841\n\n\nstateresponse\n50083.0\n0.020627\n0.005171\n0.000000\n0.018163\n0.019710\n0.023048\n0.076923\n\n\nstateresponset\n50083.0\n0.021989\n0.006257\n0.000000\n0.018493\n0.021697\n0.024703\n0.111111\n\n\nstateresponsec\n50080.0\n0.017717\n0.007516\n0.000000\n0.012862\n0.019881\n0.020806\n0.052632\n\n\nstateresponsetminc\n50080.0\n0.004273\n0.009112\n-0.047619\n-0.001388\n0.001779\n0.010545\n0.111111\n\n\nperbush\n50048.0\n0.487940\n0.078735\n0.090909\n0.444444\n0.484848\n0.525253\n0.731959\n\n\nclose25\n50048.0\n0.185702\n0.388870\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nred0\n50048.0\n0.404452\n0.490791\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\nblue0\n50048.0\n0.595548\n0.490791\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nredcty\n49978.0\n0.510245\n0.499900\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\nbluecty\n49978.0\n0.488715\n0.499878\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\npwhite\n48217.0\n0.819599\n0.168561\n0.009418\n0.755845\n0.872797\n0.938827\n1.000000\n\n\npblack\n48047.0\n0.086710\n0.135868\n0.000000\n0.014729\n0.036554\n0.090882\n0.989622\n\n\npage18_39\n48217.0\n0.321694\n0.103039\n0.000000\n0.258311\n0.305534\n0.369132\n0.997544\n\n\nave_hh_sz\n48221.0\n2.429012\n0.378115\n0.000000\n2.210000\n2.440000\n2.660000\n5.270000\n\n\nmedian_hhincome\n48209.0\n54815.700533\n22027.316665\n5000.000000\n39181.000000\n50673.000000\n66005.000000\n200001.000000\n\n\npowner\n48214.0\n0.669418\n0.193405\n0.000000\n0.560222\n0.712296\n0.816798\n1.000000\n\n\npsch_atlstba\n48215.0\n0.391661\n0.186599\n0.000000\n0.235647\n0.373744\n0.530036\n1.000000\n\n\npop_propurban\n48217.0\n0.871968\n0.258654\n0.000000\n0.884929\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\ntest_variables = ['hpa', 'freq', 'years', 'median_hhincome']\n\nfor var in test_variables:\n    control = df.loc[df.treatment == 0, var].dropna()\n    treatment = df.loc[df.treatment == 1, var].dropna()\n\n    t_stat_manual = np.divide(\n        (control.mean() - treatment.mean()),\n        np.sqrt(\n            (treatment.var(ddof=1) / len(treatment)) +\n            (control.var(ddof=1) / len(control))\n        )\n    )\n    pval_manual = 2 * (1 - t.cdf(np.abs(t_stat_manual), df=len(control) + len(treatment) - 2))\n\n    t_stat, pval = ttest_ind(control, treatment)\n    print(f'''\nT-test Results for {var}:\n\nControl mean: {control.mean()}\nTreatment mean: {treatment.mean()}\n\nt-statistic (manual): {t_stat_manual}\nt-statistic (scipy): {t_stat}\np-value (manual): {pval_manual}\np-value (scipy): {pval}\n''')\n    m = rsm.model.regress(data=df,\n                      rvar=var,\n                      evar=['treatment'],\n    )\n    print(f'Linear Regression Results for {var}:')\n\n    print(m.summary(main=False))\n    print(f'''\nAt the 95% confidence level, we {pval &lt; 0.05 and \"reject\" or \"fail to reject\"} the null hypothesis \nthat the mean value from the two samples are equal.\n---------------------------------------------------''')\n\n\nT-test Results for hpa:\n\nControl mean: 58.960166931152344\nTreatment mean: 59.59724044799805\n\nt-statistic (manual): -0.970427393913269\nt-statistic (scipy): -0.944145044786662\np-value (manual): 0.3318381861762616\np-value (scipy): 0.34510008823759086\n\nLinear Regression Results for hpa:\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.891 df(1, 50081), p.value 0.345\nNr obs: 50,083\nNone\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the mean value from the two samples are equal.\n---------------------------------------------------\n\nT-test Results for freq:\n\nControl mean: 8.047342242464193\nTreatment mean: 8.035363516588813\n\nt-statistic (manual): 0.11084502380904246\nt-statistic (scipy): 0.11089297035979982\np-value (manual): 0.9117396856546793\np-value (scipy): 0.9117016644344591\n\nLinear Regression Results for freq:\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.012 df(1, 50081), p.value 0.912\nNr obs: 50,083\nNone\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the mean value from the two samples are equal.\n---------------------------------------------------\n\nT-test Results for years:\n\nControl mean: 6.1359141846946725\nTreatment mean: 6.078365024704297\n\nt-statistic (manual): 1.0909175279574652\nt-statistic (scipy): 1.103038374578911\np-value (manual): 0.2753144222755779\np-value (scipy): 0.2700158010872445\n\nLinear Regression Results for years:\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 1.217 df(1, 50080), p.value 0.27\nNr obs: 50,082 (1 obs. dropped)\nNone\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the mean value from the two samples are equal.\n---------------------------------------------------\n\nT-test Results for median_hhincome:\n\nControl mean: 54921.09447493141\nTreatment mean: 54763.168992633575\n\nt-statistic (manual): 0.743296051066039\nt-statistic (scipy): 0.741683012117828\np-value (manual): 0.45730608418533203\np-value (scipy): 0.458283028000566\n\nLinear Regression Results for median_hhincome:\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.55 df(1, 48207), p.value 0.458\nNr obs: 48,209 (1,874 obs. dropped)\nNone\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the mean value from the two samples are equal.\n---------------------------------------------------\n\n\nConfirmed through manual t-test calculation, t-tests performed with scipy, and with linear regression models, none of the assessed variables are statistically significantly different between the control and treatment groups."
  },
  {
    "objectID": "projects/school/MGTA495/hw1.html#experimental-results",
    "href": "projects/school/MGTA495/hw1.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\ngave_df = df.groupby('treatment')['gave'].mean()\ngave_df.index = gave_df.index.map({0: 'Control', 1: 'Treatment'})\n\nplt.bar(gave_df.index, gave_df.values)\nplt.title('Proportion Who Donated by Group')\nplt.xlabel('Group')\nplt.ylabel('Proportion Who Donated')\nfor i, v in enumerate(gave_df.values):\n    plt.text(i, v-0.001, f\"{v:.3f}\", ha='center', color='white')\nplt.show()\n\n\n\n\n\n\n\n\n\ncontrol = df.loc[df.treatment == 0, 'gave'].dropna()\ntreatment = df.loc[df.treatment == 1, 'gave'].dropna()\n\nt_stat, pval = ttest_ind(control, treatment)\n\nprint(f'''\nT-test Results: \n\nControl mean: {control.mean()}\nTreatment mean: {treatment.mean()}\n\nt-statistic: {t_stat}\np-value: {pval}''')\n\nprobit_model = smf.probit('gave ~ treatment', data=df).fit(disp=False)\nprint(f'''\nProbit Regression Results:\n      \nt-statistic: {probit_model.tvalues['treatment']}\np-value: {probit_model.pvalues['treatment']}\n\nAt the 95% confidence level, we {pval &lt; 0.05 and \"reject\" or \"fail to reject\"} the null hypothesis \nthat the mean value from the two samples are equal.\n''')\n\n\nT-test Results: \n\nControl mean: 0.017858212980164198\nTreatment mean: 0.02203856749311295\n\nt-statistic: -3.101361000543946\np-value: 0.0019274025949016982\n\nProbit Regression Results:\n\nt-statistic: 3.1129300737953645\np-value: 0.0018523990147762028\n\nAt the 95% confidence level, we reject the null hypothesis \nthat the mean value from the two samples are equal.\n\n\n\nFrom both the t-test and the probit regression model, we find a statistically significant difference between the response rates (average value for binary ‘gave’ variable) from the control and treatment samples at the 99% confidence level. From this, we can conclude that potential donors are more likely to respond to solicitation and donate when provided an offer of matching donation contributions.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ndf['ratio'] = df['ratio'].astype(str)\ndf['ratio'] = pd.Categorical(\n    df['ratio'], categories=['Control', '1', '2', '3'], ordered=True\n)\nratio_df = df.groupby('ratio', observed=False)['gave'].mean()\n\nratios = ratio_df.index.values[1:]\nfor i in range(len((ratios))):\n\n    r1 = ratios[i]\n    r2 = ratios[i + 1] if i + 1 &lt; len(ratios) else ratios[0]\n\n    group1 = df.loc[df.ratio == r1, 'gave'].dropna()\n    group2 = df.loc[df.ratio == r2, 'gave'].dropna()\n\n    t_stat, pval = ttest_ind(group1, group2)\n    print(f'''\nResponse rate for {r1}:1  {group1.mean()}\nResponse rate for {r2}:1  {group2.mean()}\n\nt-statistic: {t_stat}\np-value: {pval}\n\nAt the 95% confidence level, we {pval &lt; 0.05 and \"reject\" or \"fail to reject\"} the null hypothesis \nthat the response rate of ratios {r1}:1 and {r2}:1 are equal.\n---------------------------------------------------''')\n\n\nResponse rate for 1:1  0.020749124225276205\nResponse rate for 2:1  0.0226333752469912\n\nt-statistic: -0.96504713432247\np-value: 0.33453168549723933\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the response rate of ratios 1:1 and 2:1 are equal.\n---------------------------------------------------\n\nResponse rate for 2:1  0.0226333752469912\nResponse rate for 3:1  0.022733399227244138\n\nt-statistic: -0.05011583793874515\np-value: 0.9600305283739325\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the response rate of ratios 2:1 and 3:1 are equal.\n---------------------------------------------------\n\nResponse rate for 3:1  0.022733399227244138\nResponse rate for 1:1  0.020749124225276205\n\nt-statistic: 1.0150255853798622\np-value: 0.31010466370866724\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the response rate of ratios 3:1 and 1:1 are equal.\n---------------------------------------------------\n\n\nAs seen through the above series of t-tests, comparing the mean response rates between varying match ratio samples, increasing match ratios above 1:1 has no significant impact on response rates. This affirms the conclusion made by the authors of the study.\n\nResponse Rate Regressed on Match Ratio (from full dataset):\n\nprobit_model2 = smf.probit('gave ~ ratio', data=df).fit(disp=False)\n\npm2_df = pd.DataFrame({\n    'coef': probit_model2.params,\n    't-stat': probit_model2.tvalues,\n    'pval': probit_model2.pvalues\n})\ndisplay(pm2_df)\n\n\n\n\n\n\n\n\ncoef\nt-stat\npval\n\n\n\n\nIntercept\n-2.100141\n-90.072770\n0.000000\n\n\nratio[T.1]\n0.061624\n1.725748\n0.084393\n\n\nratio[T.2]\n0.097974\n2.792255\n0.005234\n\n\nratio[T.3]\n0.099831\n2.847311\n0.004409\n\n\n\n\n\n\n\nThe large negative coefficient on the intercept demonstrates the relatively low probability of response for the control group, where the positive coefficients combined with low p-values, particularly for 2:1 and 3:1 match rates, demonstrate the statistically significant increase that the treatment has on respones rates. To further asses the difference between match rates, I will run the regression again, excluding the control group.\n\n\nResponse Rate Regressed on Match Ratio (from treatment group only):\n\ntreatment_df = df.loc[df.treatment == 1]\ntreatment_df = treatment_df[['gave', 'ratio']].dropna()\ntreatment_df['ratio'] = pd.Categorical(\n    treatment_df['ratio'], categories=['1', '2', '3'], ordered=True\n)\n\nprobit_model3 = smf.probit('gave ~ ratio', data=treatment_df).fit(disp=False)\n\npm3_df = pd.DataFrame({\n    'coef': probit_model3.params,\n    't-stat': probit_model3.tvalues,\n    'pval': probit_model3.pvalues\n})\ndisplay(pm3_df)\n\n\n\n\n\n\n\n\ncoef\nt-stat\npval\n\n\n\n\nIntercept\n-2.038517\n-75.373213\n0.000000\n\n\nratio[T.2]\n0.036350\n0.964972\n0.334559\n\n\nratio[T.3]\n0.038207\n1.014933\n0.310138\n\n\n\n\n\n\n\nWith the control group excluded, we can see from the high p-values on the coefficients for 2:1 and 3:1 match ratios that they do not have a significantly different impact on response rates from the intercept (1:1).\n\nprint(f\"Observed difference between 3:1 response rate and 2:1 response rate: {ratio_df['3'] - ratio_df['2']}\")\nprint(f\"Observed difference between 2:1 response rate and 1:1 response rate: {ratio_df['2'] - ratio_df['1']}\")\n\nmfx = probit_model2.get_margeff()\nmarginal_effects = mfx.margeff\ndiff_3_vs_2 = marginal_effects[2] - marginal_effects[1]\ndiff_2_vs_1 = marginal_effects[1] - marginal_effects[0]\n\nprint(f\"Estimated difference between 3:1 response rate and 2:1 response rate: {diff_3_vs_2}\")\nprint(f\"Estimated difference between 2:1 response rate and 1:1 response rate: {diff_2_vs_1}\")\n\nObserved difference between 3:1 response rate and 2:1 response rate: 0.00010002398025293902\nObserved difference between 2:1 response rate and 1:1 response rate: 0.0018842510217149944\nEstimated difference between 3:1 response rate and 2:1 response rate: 9.229335508974951e-05\nEstimated difference between 2:1 response rate and 1:1 response rate: 0.0018064011975636122\n\n\nThe analysis continues to affirm the authors’ findings that beyond adding a matching donation of any size, increasing match ratios is not an effective method of driving higher response rates.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\ncontrol = df.loc[df.treatment == 0, 'amount'].dropna()\ntreatment = df.loc[df.treatment == 1, 'amount'].dropna()\n\nt_stat, pval = ttest_ind(control, treatment)\n\nprint(f'''\nT-test Results:\n      \nControl mean: ${control.mean():,.2f}\nTreatment mean: ${treatment.mean():,.2f}\n      \nt-statistic: {t_stat}\np-value: {pval}\n\nAt the 95% confidence level, we {pval &lt; 0.05 and \"reject\" or \"fail to reject\"} the null hypothesis \nthat the mean donation amount of the two samples are equal.''')\n\n\nT-test Results:\n\nControl mean: $0.81\nTreatment mean: $0.97\n\nt-statistic: -1.8605020225753781\np-value: 0.06282038947470686\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the mean donation amount of the two samples are equal.\n\n\nWe can observe that the treatment group has a higher mean donation amount, as identified by the authors, but at the 95% confidence level, it is not statistically significant.\n\ncontrol = df.loc[(df.treatment == 0) & (df.gave == 1), 'amount'].dropna()\ntreatment = df.loc[(df.treatment == 1) & (df.gave == 1), 'amount'].dropna()\n\nt_stat, pval = ttest_ind(control, treatment)\n\nprint(f\"\"\"\nT-test Results:\n      \nControl mean: ${control.mean():,.2f}\nTreatment mean: ${treatment.mean():,.2f}\n\nt-statistic: {t_stat}\np-value: {pval}\n\nAt the 95% confidence level, we {pval &lt; 0.05 and 'reject' or 'fail to reject'} the null hypothesis \nthat the donation amount of the two samples are equal.\"\"\")\n\n\nT-test Results:\n\nControl mean: $45.54\nTreatment mean: $43.87\n\nt-statistic: 0.5808388615237938\np-value: 0.5614758782284279\n\nAt the 95% confidence level, we fail to reject the null hypothesis \nthat the donation amount of the two samples are equal.\n\n\nAssessing only those who made a donation, we in fact see that the mean donation amount among the treatment group is lower than the control group, though with a much higher p-value than the previous t-test. It appears that while the treatment results in higher response rates, most of those donors who are ‘converted’ by the treatment are making smaller donations than those who donated from the control group.\n\nfig, axes = plt.subplots(1, 2, figsize=(7.8, 3.9), sharey=True)\naxes[0].set_ylabel(\"Frequency\")\n\nfor val in [0, 1]:\n    subset = df.loc[(df.gave == 1) & (df.treatment == val)].copy()\n    axes[val].hist(subset['amount'], bins=10)\n    axes[val].axvline(x=subset['amount'].mean(), color='r', linestyle='--', label=f'Mean: ${subset[\"amount\"].mean():,.2f}')\n    axes[val].legend()\n    axes[val].set_title(f'Amount Donated for {\"Treatment\" if val == 1 else \"Control\"} Group')\n    axes[val].set_xlabel(\"Dollars Donated\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/school/MGTA495/hw1.html#simulation-experiment",
    "href": "projects/school/MGTA495/hw1.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThe below chart illustrates the cumulative average difference in response rates from 10,000 draws of simulated treatment and control distributions. In early simulations, we see this cumulative average move erratically due to random variation, but as more draws are simulated, the average begins to stabilize and converge toward the true observed treatment effect of approximately 0.004.\n\nctr_p = 0.018\ntrt_p = 0.022\n\nnp.random.seed(12)\nsim_ctr = np.random.binomial(n=1, p=ctr_p, size=10_000)\nsim_trt = np.random.binomial(n=1, p=trt_p, size=10_000)\n\nsim_diff = sim_trt - sim_ctr\ncumulative_avg = np.cumsum(sim_diff) / np.arange(1, len(sim_diff) + 1)\n\nplt.figure(figsize=(7.8, 3.9))\nplt.plot(cumulative_avg)\nplt.axhline(y=trt_p - ctr_p, color='red', linestyle='--', label=f'True Effect ({trt_p-ctr_p:.3f})')\nplt.title('Cumulative Average of Simulated Treatment Effect')\nplt.xlabel('Number of Simulated Draws')\nplt.ylabel('Cum. Avg. Difference (Treatment - Control)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nThese histograms show the distribution of simulated average differences in response rates between treatment and control groups at sample sizes of 50, 200, 500, and 1000. As the sample size increases, the distributions become tighter and more symmetric, demonstrating the central limit theorem: larger samples yield more stable, normally distributed estimates.\n\nfig, axes = plt.subplots(2, 2, figsize=(7.8, 7.8), sharey=True, sharex=True)\naxes = axes.flatten()\naxes[0].set_ylabel(\"Frequency\")\naxes[2].set_ylabel(\"Frequency\")\naxes[2].set_xlabel(\"Difference in Response Rate\")\naxes[3].set_xlabel(\"Difference in Response Rate\")\n\nfor ax, size in enumerate([50, 200, 500, 1000]):\n    samples = []\n    for i in range(1000):\n        sim_ctr = np.random.binomial(n=1, p=ctr_p, size=size)\n        sim_trt = np.random.binomial(n=1, p=trt_p, size=size)\n\n        sim_diff_mean = sim_trt.mean() - sim_ctr.mean()\n        samples.append(sim_diff_mean)\n    \n    axes[ax].hist(samples, bins=np.linspace(-0.02, 0.03, 11))\n    axes[ax].axvline(x=trt_p - ctr_p, color='red', linestyle='--', label='True Effect (0.004)')\n    axes[ax].legend()\n    axes[ax].set_title(f'Average Difference from {size} Draws')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "code/old_code.html",
    "href": "code/old_code.html",
    "title": "Wesley Covey",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "bonus.html",
    "href": "bonus.html",
    "title": "Rex",
    "section": "",
    "text": "🦖🌵\n\n\nScore: 0\n\n\n\n Jump!"
  },
  {
    "objectID": "archive/burritos.html",
    "href": "archive/burritos.html",
    "title": "Example Power BI Report",
    "section": "",
    "text": "Project Notes\nAs part of an early brainstorming idea for a project for UCSD’s Collecting and Analyzing Large Data course, I pulled a dataset from Kaggle on burrito reviews in the San Diego area. It’s a bit silly, and we didn’t end up moving forward with this project, but I created the below Power BI report with the data as a proof of concept."
  },
  {
    "objectID": "code/code_test.html",
    "href": "code/code_test.html",
    "title": "Wesley Covey",
    "section": "",
    "text": "import hashlib\n\npw = 'bigbear'\nhpw = hashlib.sha256(pw.encode()).hexdigest()\nprint(hpw)\n\nf98f3d8cbcfa62aac83f1abb6411e40c5da9b2cf9ed67c0227b8675351a04500"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wesley D. Covey",
    "section": "",
    "text": "Wesley is a data-driven operations analyst and Marine Corps veteran with a proven record of turning complex data into actionable insights. With experience spanning financial analytics, business intelligence, and process optimization, he brings precision and purpose to every challenge. Outside of work, he enjoys gardening, woodworking, and exploring San Diego’s hiking trails.\n\n\nUniversity of California, San Diego\nRady School of Management\nM.S. in Business Analytics | 2024 - Present\nArizona State University\nW.P. Carey School of Business\nB.S. in Economics | 2018 - 2023\n\n\n\nAcademy Securities\nOperations Analyst | 2023 - Present\nUnited States Marine Corps\nChief Force Deployment Analyst | 2015 - 2023"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Wesley D. Covey",
    "section": "",
    "text": "University of California, San Diego\nRady School of Management\nM.S. in Business Analytics | 2024 - Present\nArizona State University\nW.P. Carey School of Business\nB.S. in Economics | 2018 - 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Wesley D. Covey",
    "section": "",
    "text": "Academy Securities\nOperations Analyst | 2023 - Present\nUnited States Marine Corps\nChief Force Deployment Analyst | 2015 - 2023"
  },
  {
    "objectID": "projects/school/MGTA495/hw2.html",
    "href": "projects/school/MGTA495/hw2.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n[1] \"Non-Customer mean:  3.4730127576055\"\n\n\n[1] \"Customer mean:  4.13305613305613\"\n\n\n\n\n\n\n\n\n\nThe histograms of patent counts by customers vs non-customers reveal nearly identical distributions with a slightly higher mean value for customers of the firm.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe company’s customers are disproportionately from the Northeast region, though not entirely. The distibution of age is, howver, much more even for customer vs non-customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n                    Estimate    Std_Error\n(Intercept)     -0.125735914 0.1122180346\nage              0.115793715 0.0063574229\nage_sq          -0.002228748 0.0000771291\nregionNortheast -0.024556782 0.0433762879\nregionNorthwest -0.034827790 0.0529311002\nregionSouth     -0.005441860 0.0524007440\nregionSouthwest -0.037784109 0.0471722463\niscustomer       0.060665584 0.0320588299"
  },
  {
    "objectID": "projects/school/MGTA495/hw2.html#blueprinty-case-study",
    "href": "projects/school/MGTA495/hw2.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n[1] \"Non-Customer mean:  3.4730127576055\"\n\n\n[1] \"Customer mean:  4.13305613305613\"\n\n\n\n\n\n\n\n\n\nThe histograms of patent counts by customers vs non-customers reveal nearly identical distributions with a slightly higher mean value for customers of the firm.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe company’s customers are disproportionately from the Northeast region, though not entirely. The distibution of age is, howver, much more even for customer vs non-customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n                    Estimate    Std_Error\n(Intercept)     -0.125735914 0.1122180346\nage              0.115793715 0.0063574229\nage_sq          -0.002228748 0.0000771291\nregionNortheast -0.024556782 0.0433762879\nregionNorthwest -0.034827790 0.0529311002\nregionSouth     -0.005441860 0.0524007440\nregionSouthwest -0.037784109 0.0471722463\niscustomer       0.060665584 0.0320588299"
  },
  {
    "objectID": "projects/school/MGTA495/hw2.html#airbnb-case-study",
    "href": "projects/school/MGTA495/hw2.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\n       X               id                days       last_scraped      \n Min.   :    1   Min.   :    2515   Min.   :    1   Length:40628      \n 1st Qu.:10158   1st Qu.: 4889868   1st Qu.:  542   Class :character  \n Median :20315   Median : 9862878   Median :  996   Mode  :character  \n Mean   :20315   Mean   : 9698889   Mean   : 1102                     \n 3rd Qu.:30471   3rd Qu.:14667894   3rd Qu.: 1535                     \n Max.   :40628   Max.   :18009669   Max.   :42828                     \n                                                                      \n  host_since         room_type           bathrooms        bedrooms     \n Length:40628       Length:40628       Min.   :0.000   Min.   : 0.000  \n Class :character   Class :character   1st Qu.:1.000   1st Qu.: 1.000  \n Mode  :character   Mode  :character   Median :1.000   Median : 1.000  \n                                       Mean   :1.125   Mean   : 1.147  \n                                       3rd Qu.:1.000   3rd Qu.: 1.000  \n                                       Max.   :8.000   Max.   :10.000  \n                                       NA's   :160     NA's   :76      \n     price         number_of_reviews review_scores_cleanliness\n Min.   :   10.0   Min.   :  0.0     Min.   : 2.000           \n 1st Qu.:   70.0   1st Qu.:  1.0     1st Qu.: 9.000           \n Median :  100.0   Median :  4.0     Median :10.000           \n Mean   :  144.8   Mean   : 15.9     Mean   : 9.198           \n 3rd Qu.:  170.0   3rd Qu.: 17.0     3rd Qu.:10.000           \n Max.   :10000.0   Max.   :421.0     Max.   :10.000           \n                                     NA's   :10195            \n review_scores_location review_scores_value instant_bookable  \n Min.   : 2.000         Min.   : 2.000      Length:40628      \n 1st Qu.: 9.000         1st Qu.: 9.000      Class :character  \n Median :10.000         Median :10.000      Mode  :character  \n Mean   : 9.414         Mean   : 9.332                        \n 3rd Qu.:10.000         3rd Qu.:10.000                        \n Max.   :10.000         Max.   :10.000                        \n NA's   :10254          NA's   :10256                         \n\n\n\nCall:\nglm(formula = number_of_reviews ~ room_type + bathrooms + bedrooms + \n    price + review_scores_cleanliness + review_scores_location + \n    review_scores_value + instant_bookable, family = poisson(), \n    data = df_clean)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.572e+00  1.600e-02 223.215  &lt; 2e-16 ***\nroom_typePrivate room     -1.453e-02  2.737e-03  -5.310 1.09e-07 ***\nroom_typeShared room      -2.519e-01  8.618e-03 -29.229  &lt; 2e-16 ***\nbathrooms                 -1.240e-01  3.747e-03 -33.091  &lt; 2e-16 ***\nbedrooms                   7.494e-02  1.988e-03  37.698  &lt; 2e-16 ***\nprice                     -1.436e-05  8.303e-06  -1.729   0.0838 .  \nreview_scores_cleanliness  1.132e-01  1.493e-03  75.821  &lt; 2e-16 ***\nreview_scores_location    -7.680e-02  1.607e-03 -47.796  &lt; 2e-16 ***\nreview_scores_value       -9.153e-02  1.798e-03 -50.902  &lt; 2e-16 ***\ninstant_bookablet          3.344e-01  2.889e-03 115.748  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 936528  on 30150  degrees of freedom\nAIC: 1058014\n\nNumber of Fisher Scoring iterations: 6\n\n\nInstant booking and higher cleanliness scores both have positive coefficients, leading to more reviews. Shared rooms and higher number of bathrooms both have negative coefficients, resulting in fewer reviews. Surprisingly, higher location and value scores are also linked to fewer reviews, though not as strong as the last two variables. The only variable not statistically significant to the model at the 95% confidence level is price."
  },
  {
    "objectID": "projects/school/shiny_real_estate.html",
    "href": "projects/school/shiny_real_estate.html",
    "title": "San Diego Real Estate Shiny App",
    "section": "",
    "text": "I created the below Shiny web app as part of a group project from my Collecting and Analyzing Large Data course at UCSD. The assignment was broad in nature, requiring that we find a large data source and use it to produce an analytical report, tool, or presentation. Our group decided to produce something to assist a San Diego resident trying to make a decision between renting and buying.\nWe assessed several options for housing market data, and ended up using a combination of public data from Redfin and Zillow to perform trend analysis on home sale activity and housing prices in the area. One of my contributions to our project was the creation of the below decision-making tool, where a user can customize their inputs and receive a calculated recommendation with a map of feasible locations (location prices generated from Zillow data as of Oct 2024).\n\nClick here to open the app in a new tab."
  },
  {
    "objectID": "projects/school/shiny_real_estate.html#project-notes",
    "href": "projects/school/shiny_real_estate.html#project-notes",
    "title": "San Diego Real Estate Shiny App",
    "section": "",
    "text": "I created the below Shiny web app as part of a group project from my Collecting and Analyzing Large Data course at UCSD. The assignment was broad in nature, requiring that we find a large data source and use it to produce an analytical report, tool, or presentation. Our group decided to produce something to assist a San Diego resident trying to make a decision between renting and buying.\nWe assessed several options for housing market data, and ended up using a combination of public data from Redfin and Zillow to perform trend analysis on home sale activity and housing prices in the area. One of my contributions to our project was the creation of the below decision-making tool, where a user can customize their inputs and receive a calculated recommendation with a map of feasible locations (location prices generated from Zillow data as of Oct 2024).\n\nClick here to open the app in a new tab."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Logos used on this site are the property of their respective owners. No endorsement or affiliation is implied. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRage Against The Machine Learning\n\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\nMay 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSan Diego Real Estate Shiny App\n\n\n\nNov 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Trade Power BI Report\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]